{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPFOVO7EKGN5RPzlbMrrtk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ricardo711/LLM/blob/master/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U7NQqsdh2ASK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bf4068-12e1-4cd5-a882-c16bed1b1e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 16])\n",
            "tensor([[ 9.7601,  1.7326,  4.7543, -1.3587,  0.4752, -1.6717,  1.0227, -0.1286],\n",
            "        [ 1.7326, 16.0787,  9.0642, -0.3370,  1.1368,  1.1972,  1.6485, -1.2789],\n",
            "        [ 4.7543,  9.0642, 22.6615, -0.8519,  7.7799,  2.7483, -0.6832,  1.6236],\n",
            "        [-1.3587, -0.3370, -0.8519, 13.9473, -1.4198, 10.9659, -0.5887,  2.3869],\n",
            "        [ 0.4752,  1.1368,  7.7799, -1.4198, 13.7511, -6.8568, -2.5114, -3.3468],\n",
            "        [-1.6717,  1.1972,  2.7483, 10.9659, -6.8568, 24.6738, -3.8294,  4.9581],\n",
            "        [ 1.0227,  1.6485, -0.6832, -0.5887, -2.5114, -3.8294, 15.8691,  2.0269],\n",
            "        [-0.1286, -1.2789,  1.6236,  2.3869, -3.3468,  4.9581,  2.0269, 18.7382]])\n",
            "torch.Size([8, 8])\n",
            "tensor([[9.9270e-01, 3.2398e-04, 6.6502e-03, 1.4723e-05, 9.2135e-05, 1.0766e-05,\n",
            "         1.5929e-04, 5.0374e-05],\n",
            "        [5.8773e-07, 9.9910e-01, 8.9788e-04, 7.4187e-08, 3.2391e-07, 3.4407e-07,\n",
            "         5.4033e-07, 2.8926e-08],\n",
            "        [1.6712e-08, 1.2438e-06, 1.0000e+00, 6.1412e-11, 3.4437e-07, 2.2482e-09,\n",
            "         7.2703e-11, 7.3008e-10],\n",
            "        [2.1438e-07, 5.9550e-07, 3.5585e-07, 9.5172e-01, 2.0167e-07, 4.8272e-02,\n",
            "         4.6299e-07, 9.0760e-06],\n",
            "        [1.7110e-06, 3.3158e-06, 2.5448e-03, 2.5719e-07, 9.9745e-01, 1.1195e-09,\n",
            "         8.6338e-08, 3.7443e-08],\n",
            "        [3.6165e-12, 6.3713e-11, 3.0052e-10, 1.1136e-06, 2.0250e-14, 1.0000e+00,\n",
            "         4.1804e-13, 2.7390e-09],\n",
            "        [3.5667e-07, 6.6694e-07, 6.4779e-08, 7.1194e-08, 1.0410e-08, 2.7865e-09,\n",
            "         1.0000e+00, 9.7366e-07],\n",
            "        [6.4013e-09, 2.0263e-09, 3.6918e-08, 7.9205e-08, 2.5622e-10, 1.0361e-06,\n",
            "         5.5258e-08, 1.0000e+00]])\n",
            "torch.Size([8, 16])\n",
            "tensor([[ 3.3420e-01, -1.8324e-01, -3.0218e-01, -5.7772e-01,  3.5662e-01,\n",
            "          6.6452e-01, -2.0998e-01, -3.7798e-01,  7.6537e-01, -1.1946e+00,\n",
            "          6.9960e-01, -1.4067e+00,  1.7021e-01,  1.8838e+00,  4.8729e-01,\n",
            "          2.4730e-01],\n",
            "        [-9.3975e-01, -4.6856e-01,  1.0311e+00, -2.8192e-01,  4.9373e-01,\n",
            "         -1.2896e-02, -2.7327e-01, -7.6358e-01,  1.3958e+00, -9.9543e-01,\n",
            "         -7.1287e-04,  1.2449e+00, -7.8077e-02,  1.2765e+00, -1.4589e+00,\n",
            "         -2.1601e+00],\n",
            "        [-7.7021e-02, -1.0205e+00, -1.6895e-01,  9.1776e-01,  1.5810e+00,\n",
            "          1.3010e+00,  1.2753e+00, -2.0095e-01,  4.9647e-01, -1.5723e+00,\n",
            "          9.6657e-01, -1.1481e+00, -1.1589e+00,  3.2547e-01, -6.3151e-01,\n",
            "         -2.8400e+00],\n",
            "        [-1.3679e+00,  1.0614e-01, -2.1317e+00,  1.0480e+00, -3.7127e-01,\n",
            "         -9.1234e-01, -4.3802e-01, -1.0329e+00,  9.3425e-01,  1.5453e+00,\n",
            "          5.7218e-01, -1.8049e-01, -6.0454e-03, -8.8691e-02,  2.0559e-01,\n",
            "         -5.2292e-01],\n",
            "        [ 2.5444e-01, -5.5082e-01,  1.0012e+00,  8.2746e-01, -3.8978e-01,\n",
            "          4.9129e-01, -2.1302e-01, -1.7432e+00, -1.5972e+00, -1.0776e+00,\n",
            "          9.0331e-01, -7.2292e-01, -5.9652e-01, -7.0857e-01,  6.1977e-01,\n",
            "         -1.3766e+00],\n",
            "        [-2.2150e+00, -1.3193e+00, -2.0915e+00,  9.6285e-01, -3.1862e-02,\n",
            "         -4.7896e-01,  7.6681e-01,  2.7467e-02,  1.9929e+00,  1.3708e+00,\n",
            "         -5.0087e-01, -2.7928e-01, -2.0628e+00,  6.3744e-03, -9.8955e-01,\n",
            "          7.0161e-01],\n",
            "        [ 5.1463e-01,  9.9376e-01, -2.5873e-01, -1.0825e+00, -4.4383e-02,\n",
            "          1.6236e+00, -2.3229e+00,  1.0878e+00,  6.7156e-01,  6.9329e-01,\n",
            "         -9.4872e-01, -7.6506e-02, -1.5264e-01,  1.1674e-01,  4.4026e-01,\n",
            "         -1.4465e+00],\n",
            "        [ 8.7683e-01,  1.6221e+00, -1.4779e+00,  1.1331e+00, -1.2203e+00,\n",
            "          1.3139e+00,  1.0533e+00,  1.3880e-01,  2.2473e+00, -8.0363e-01,\n",
            "         -2.8084e-01,  7.6967e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,\n",
            "          2.2935e-01]])\n"
          ]
        }
      ],
      "source": [
        "#lets compute wij\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "#input_id tokenizer\n",
        "sentence=torch.tensor([0,#can\n",
        "                       7,#you\n",
        "                       1,#help\n",
        "                       2,#me\n",
        "                       5,#to\n",
        "                       6,#translate\n",
        "                       4,#this\n",
        "                       3]#sentence\n",
        "                      )\n",
        "#encoded this sentence into a real-number vector representation via embedding\n",
        "torch.manual_seed(123)\n",
        "embed=torch.nn.Embedding(num_embeddings=10,embedding_dim=16)\n",
        "embedded_sentence=embed(sentence).detach()#detach means require_grad=False\n",
        "print(embedded_sentence.shape)\n",
        "\n",
        "#now we can compute wij as the dot product between the ith and jth word embeddings\n",
        "omega=torch.empty(8,8)\n",
        "for i,x_i in enumerate(embedded_sentence):\n",
        "  for j,x_j in enumerate(embedded_sentence):\n",
        "    omega[i,j]=torch.dot(x_i,x_j)\n",
        "#################################################################3\n",
        "#Step1: compute dot products\n",
        "################################################################33\n",
        "#2 for loops are ineficcient. using matmul to matrix multiplication\n",
        "omega_mat=embedded_sentence.matmul(embedded_sentence.T)#X*XT\n",
        "print(omega_mat)\n",
        "#omega_mat[0,5] = -1.6717 # similarity between \"can\" and \"translate\"\n",
        "#omega_mat[2,1] = 9.0642  # similarity between \"help\" and \"you\"\n",
        "\n",
        "#we can use torch.allclose to check if both are the same\n",
        "torch.allclose(omega,omega_mat)\n",
        "\n",
        "#we have compute the similarity-based weights for the ith input and all inputs in the sequence\n",
        "#we can obtain the attention weights aij by normalizing the wij via softmax function\n",
        "####################################################################################\n",
        "#step 2: normalize using softmax\n",
        "####################################################################################3\n",
        "attention_weights=F.softmax(omega,dim=1)\n",
        "print(attention_weights.shape)\n",
        "print(attention_weights)\n",
        "# Row 0: The 0th token mostly attends to itself (0.9927).\n",
        "# Very little attention is given to:\n",
        "# - 1st token: 0.000324\n",
        "# - 2nd token: 0.00665\n",
        "# - 3rd token: 0.0000147\n",
        "# - 4th token: 0.0000921\n",
        "# - 5th token: 0.0000108\n",
        "# - 6th token: 0.000159\n",
        "# - 7th token: 0.0000504\n",
        "\n",
        "#these attention weights indicate how relevant each word is to the ith word.\n",
        "#columns must sum1\n",
        "attention_weights.sum(dim=1)\n",
        "\n",
        "#########################################################################################\n",
        "#step3: compute output context aware embedding vector\n",
        "#######################################################################################\n",
        "context_vectors=torch.matmul(attention_weights,embedded_sentence)\n",
        "print(context_vectors.shape)\n",
        "print(context_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameterizing the self-attention mechanism: scaled dot product attention"
      ],
      "metadata": {
        "id": "Vd1i1RyIsjh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3 weight matrices Uq,Uk,Uv. qi=Uqxi,ki=Ukxi,vi=Uvxi. Uq and Uk have shape dk x d, while Uv dv x d (d is embedding dimmesnion). For simplicity dk=dv=d\n",
        "torch.manual_seed(123)\n",
        "d=embedded_sentence.shape[1]\n",
        "U_query=torch.rand(d,d)#uniform distribution [0,1)\n",
        "U_key=torch.rand(d,d)\n",
        "U_value=torch.rand(d,d)\n",
        "#we will use the embedding representation of token 2 (you)\n",
        "x_2=embedded_sentence[1]\n",
        "query_2=U_query.matmul(x_2)\n",
        "key_2=U_key.matmul(x_2)\n",
        "value_2=U_value.matmul(x_2)\n",
        "#we also need the key and value sequences for all other inputs\n",
        "keys=U_key.matmul(embedded_sentence.T).T\n",
        "values=U_value.matmul(embedded_sentence.T).T\n",
        "#confirm everything is correct\n",
        "print(torch.allclose(keys[1],key_2))\n",
        "print(torch.allclose(values[1],value_2))\n",
        "\n",
        "################################################\n",
        "#step1: compute wij=qiT*kj\n",
        "###############################################\n",
        "omega_2=query_2.matmul(keys.T)\n",
        "print(omega_2)\n",
        "\n",
        "#################################################\n",
        "#step2\" normalized attention weights using softmax and scaling factor\n",
        "##################################################\n",
        "attention_weights_2=F.softmax(omega_2/d**0.5,dim=0)\n",
        "print(attention_weights_2)\n",
        "\n",
        "#######################################################\n",
        "#step3: weighted average of value sequence\n",
        "#############################################33333\n",
        "context_vector_2=attention_weights_2.matmul(values)\n",
        "print(context_vector_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oAlUm6HsxOd",
        "outputId": "2ff9a6e8-8fe2-4a7b-c581-81b91309e2d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "tensor([-25.1623,   9.3602,  14.3667,  32.1482,  53.8976,  46.6626,  -1.2131,\n",
            "        -32.9392])\n",
            "tensor([2.2317e-09, 1.2499e-05, 4.3696e-05, 3.7242e-03, 8.5596e-01, 1.4026e-01,\n",
            "        8.8897e-07, 3.1935e-10])\n",
            "tensor([-1.2226, -3.4387, -4.3928, -5.2125, -1.1249, -3.3041, -1.4316, -3.2765,\n",
            "        -2.5114, -2.6105, -1.5793, -2.8433, -2.4142, -0.3998, -1.9917, -3.3499])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#original transformer\n"
      ],
      "metadata": {
        "id": "pu_uSzgnh2bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder take input xi,...xn and map an output sequence representation z1,...zn\n",
        "#multihead attention we use Uq,Uv,Uk, the set of that 3 are called attention head\n",
        "#given input as each element embedded of size d, we can a set of matrix Uq1,UK1,uV1,Uq2,Uk2,Uv2....Uqh,Ukh,Uvh\n",
        "#both Uqj and Ukj have shape dk x d and Uvj has dimension dv x d. For simplicity dk=dv=d\n",
        "torch.manual_seed(123)\n",
        "d=embedded_sentence.shape[1]\n",
        "one_Q_query=torch.rand(d,d)\n",
        "#suposse we have 8 attention heads\n",
        "h=8\n",
        "multihead_U_query=torch.rand(h,d,d)\n",
        "multihead_U_key=torch.rand(h,d,d)\n",
        "multihead_U_value=torch.rand(h,d,d)\n",
        "print(multihead_U_key.shape)\n",
        "\n",
        "#################################################3\n",
        "#step1 compute qij=Uqjxi\n",
        "##################################################\n",
        "#we will need to use .reapeat() because there are 8 attention heads\n",
        "stacked_inputs=embedded_sentence.T.repeat(8,1,1)\n",
        "print(stacked_inputs.shape)\n",
        "#batch multiplication via torch.bmm() to compute all keys\n",
        "multihead_keys=torch.bmm(multihead_U_key,stacked_inputs)\n",
        "print(multihead_keys.shape) #[no.attention heads,embedding dimension,no.words]\n",
        "#swapping 2nd and 3rd dimension\n",
        "multihead_keys=multihead_keys.permute(0,2,1)\n",
        "print(multihead_keys.shape)\n",
        "\n",
        "#for values\n",
        "multihead_values=torch.matmul(multihead_U_value,stacked_inputs)\n",
        "multihead_values=multihead_values.permute(0,2,1)\n",
        "print(multihead_values.shape)\n",
        "\n",
        "#######################################################3\n",
        "#calculate context vectors\n",
        "#####################################################\n",
        "import math\n",
        "#step 1\n",
        "multihead_queries = torch.bmm(multihead_U_query, stacked_inputs)  # shape: [h, d, T]\n",
        "multihead_queries = multihead_queries.permute(0, 2, 1)\n",
        "print(multihead_queries.shape)\n",
        "\n",
        "# Step 2: compute dot product QK^T\n",
        "# multihead_queries: [h, T, d]\n",
        "# multihead_keys:    [h, T, d]\n",
        "# we need to compute: Q @ K.T => [h, T, T]\n",
        "scores = torch.bmm(multihead_queries, multihead_keys.transpose(1, 2))\n",
        "\n",
        "# Step 3: scale the scores\n",
        "dk = d  # assuming dk = d\n",
        "scaled_scores = scores / math.sqrt(dk)\n",
        "\n",
        "# Step 4: apply softmax to get attention weights\n",
        "attention_weights = torch.softmax(scaled_scores, dim=-1)\n",
        "\n",
        "# Step 5: multiply with values to get context vectors\n",
        "# multihead_values: [h, T, d]\n",
        "context_vectors = torch.bmm(attention_weights, multihead_values)  # shape: [h, T, d]\n",
        "print(context_vectors.shape)\n",
        "print(context_vectors[0][0]) #The context vector of the first token (index 0) computed by the first attention head (head 0).\n",
        "#Interpreting the dimensions:\n",
        "#First dimension (8): Number of attention heads.\n",
        "#Second dimension (8): Number of tokens (words) in the input sentence.\n",
        "#Third dimension (16): Context vector dimension (same as your embedding dim d=16).\n",
        "\n",
        "#context_vectors[head][token] is a vector of size 16 that represents what\n",
        "#that attention head thinks is important about the token at that position in the sentence — after looking at all other tokens.\n",
        "\n",
        "\n",
        "#################################################################3\n",
        "#linear projection\n",
        "#################################################################\n",
        "linear=torch.nn.Linear(8*8*16,16)\n",
        "context_vector=linear(context_vectors.flatten())\n",
        "print(context_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84rFbgvWh4Uc",
        "outputId": "28c385fd-8300-4475-d226-b2b5bee69dd6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 16, 16])\n",
            "torch.Size([8, 16, 8])\n",
            "torch.Size([8, 16, 8])\n",
            "torch.Size([8, 8, 16])\n",
            "torch.Size([8, 8, 16])\n",
            "torch.Size([8, 8, 16])\n",
            "torch.Size([8, 8, 16])\n",
            "tensor([ 4.5022,  3.0754,  3.6300,  0.7366,  0.7970,  2.5551,  4.0832,  1.2459,\n",
            "         0.8504,  1.7613,  0.7076,  2.1336,  0.5397, -0.0704,  1.2788,  2.4048])\n",
            "torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTM5ok1MkuOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}